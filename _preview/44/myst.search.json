{"version":"1","records":[{"hierarchy":{"lvl1":"HRRR on AWS Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nIn this Project Pythia Cookbook, you will access and create a map from archived data from NCEP’s High-Resolution Rapid Refresh (HRRR) model, which is served in an S3 bucket on AWS.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Motivation"},"content":"This cookbook provides the essential materials to learning how to work with gridded NCEP model output that is served on AWS’ S3 buckets, in a data format called Zarr.\n\nOnce you go through this material, you will have mastered the following skills:\n\nUnderstand what object store refers to, and how it relates to AWS’s S3 buckets\n\nFamiliarized yourself with the Zarr data representation model, and why it is an optimal format for data stored on S3\n\nAccess, analyze, and visualize gridded fields from the HRRR\n\nThroughout this cookbook, we build on the core foundational Python material covered in the \n\nFoundations Book","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Authors"},"content":"Kevin Tyle","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Structure"},"content":"This cookbook will have two main sections - “Foundations” and “Example Workflows.”","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Foundations","lvl2":"Structure"},"type":"lvl3","url":"/#foundations","position":10},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Foundations","lvl2":"Structure"},"content":"Currently under development\nThe foundational content will include:\n\nNCEP Model data on AWS’ S3\n\nan overview of how to access NCEP’s real-time and archived NWP model output on AWS\n\nan introduction to the Zarr data format\n\nHow to read in a Zarr-formatted HRRR grid with Xarray","type":"content","url":"/#foundations","position":11},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Example Workflows","lvl2":"Structure"},"type":"lvl3","url":"/#example-workflows","position":12},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Example Workflows","lvl2":"Structure"},"content":"Here, we apply the lessons learned in the foundational material to various analysis workflows, including everything from reading in the data to plotting a beautiful visualization at the end. We include the additional dataset-specific details, focusing on building upon the foundational materials rather than duplicating previous content.\n\nPlot a map of 2-meter temperature from a past HRRR run\n\nCurrently under development Plot a time series of wind speed from a past HRRR run\n\nCurrently under development Plot a sequence of forecast maps for the most recent run of the HRRR","type":"content","url":"/#example-workflows","position":13},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nFoundations book chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"HRRR on AWS Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the \n\n“HRRR-AWS-cookbook” repositorygit clone https://github.com/ProjectPythia/HRRR-AWS-cookbook.git\n\nMove into the HRRR-AWS-cookbook directorycd HRRR-AWS-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate HRRR-AWS-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab\n\nAt this point, you can interact with the notebooks! Make sure to check out the \n\n“Getting Started with Jupyter” content from the \n\nPythia Foundations material if you are new to Jupyter or need a refresher.","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures"},"type":"lvl1","url":"/notebooks/example-workflows/plot-2mt","position":0},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt","position":1},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures"},"type":"lvl1","url":"/notebooks/example-workflows/plot-2mt#plotting-hrrr-2-meter-temperatures","position":2},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#plotting-hrrr-2-meter-temperatures","position":3},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#overview","position":4},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Overview"},"content":"Access archived HRRR data hosted on AWS in Zarr format\n\nVisualize one of the variables (2m temperature) at an analysis time\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#overview","position":5},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#prerequisites","position":6},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nXarray Lessons 1-9\n\nNecessary\n\n\n\nTime to learn: 30 minutes\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#prerequisites","position":7},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#imports","position":8},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Imports"},"content":"\n\nimport xarray as xr\nimport metpy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport pandas as pd\nimport s3fs\nfrom importlib.metadata import version\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#imports","position":9},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"What is Zarr?"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#what-is-zarr","position":10},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"What is Zarr?"},"content":"Gridded datasets, especially those produced by operational meteorological centers such as NCEP and ECMWF, are typically in NetCDF and GRIB formats. Zarr is a relatively new data format. It is particularly relevant in the following two scenarios:\n\nDatasets that are stored in what’s called object store. This is a commonly-used storage method for cloud providers, such as Amazon, Google, and Microsoft.\n\nDatasets that are typically too large to load into memory all at once.\n\nXarray supports the Zarr format in addition to NetCDF and GRIB. The \n\nPangeo project specifically recommends Zarr as the Xarray-amenable data format of choice in the cloud:\n\n“Our current preference for storing multidimensional array data in the cloud is the Zarr format. Zarr is a new storage format which, thanks to its simple yet well-designed specification, makes large datasets easily accessible to distributed computing. In Zarr datasets, the arrays are divided into chunks and compressed. These individual chunks can be stored as files on a filesystem or as objects in a cloud storage bucket. The metadata are stored in lightweight .json files. Zarr works well on both local filesystems and cloud-based object stores. Existing datasets can easily be converted to zarr via xarray’s zarr functions.”\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#what-is-zarr","position":11},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Access archived HRRR data hosted on AWS in Zarr format"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#access-archived-hrrr-data-hosted-on-aws-in-zarr-format","position":12},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Access archived HRRR data hosted on AWS in Zarr format"},"content":"\n\nFor a number of years, the \n\nMesowest group at the University of Utah has hosted an archive of data from NCEP’s High Resolution Rapid Refresh model. This data, originally in GRIB-2 format, has been converted into Zarr and is freely available “in the cloud”, on \n\nAmazon Web Service’s Simple Storage Service, otherwise known as S3. Data is stored in S3 in a manner akin to (but different from) a Linux filesystem, using a \n\nbucket and object model.\n\nTo interactively browse the contents of this archive, go to this link: \n\nHRRRZarr File Browser on AWS\n\nTo access Zarr-formatted data stored in an S3 bucket, we follow a 3-step process:\n\nCreate URL(s) pointing to the bucket and object(s) that contain the data we want\n\nCreate map(s) to the object(s) with the s3fs library’s S3Map method\n\nPass the map(s) to Xarray’s open_dataset or open_mfdataset methods, and specify zarr as the format, via the engine argument.\n\nA quirk in how these grids were converted from GRIB2 to Zarr means that the dimension variables are defined one directory up from where the data variables are. Thus, our strategy is to use Xarray's open_mfdataset method and pass in two AWS S3 file references to these two corresponding directories.\n\nCreate the URLs\n\ndate = '20210214'\nhour = '12'\nvar = 'TMP'\nlevel = '2m_above_ground'\nurl1 = 's3://hrrrzarr/sfc/' + date + '/' + date + '_' + hour + 'z_anl.zarr/' + level + '/' + var + '/' + level\nurl2 = 's3://hrrrzarr/sfc/' + date + '/' + date + '_' + hour + 'z_anl.zarr/' + level + '/' + var\n\nIn this case, hrrrzarr is the S3 bucket name. 2m_above_ground and TMP are both objects within the bucket. The former object has the 2-meter temperature array, while the latter contains the coordinate arrays of the spatial dimensions of 2m temperature (i.e., x and y).\n\nConnect to the S3 object store. With the \n\nrelease of Zarr version 3, the methods to do this have changed from version 2 (see, e.g. Zarr issues \n\n2706 and \n\n2748) . Test which version is installed and use the appropriate methodology.\n\npackage = \"zarr\"\npackage_version = version(package)\nmajor_version = int(package_version.split(\".\")[0])  # Extract the major version\n\nif major_version == 3:      \n    import zarr\n    # strip leading 's3://' from url\n    url1_3 = url1[5:]\n    url2_3 = url2[5:]\n    fs = s3fs.S3FileSystem(anon=True, asynchronous=True)\n    file1 = zarr.storage.FsspecStore(fs, path=url1_3)\n    file2 = zarr.storage.FsspecStore(fs, path=url2_3)\nelse:\n    fs = s3fs.S3FileSystem(anon=True, asynchronous=False)\n    file1 = s3fs.S3Map(url1, s3=fs)\n    file2 = s3fs.S3Map(url2, s3=fs)\n\nUse Xarray’s open_mfdataset to create a Dataset from these two S3 objects.\n\nds = xr.open_mfdataset([file1,file2], engine='zarr')\n\nExamine the dataset.\n\nds\n\nGet the map projection for the HRRR, which is stored in the grid folder in the AWS bucket.\n\nHRRR_proj = pd.read_json (\"https://hrrrzarr.s3.amazonaws.com/grid/projparams.json\", orient=\"index\")\nHRRR_proj\n\nlat_0 = HRRR_proj.loc['lat_0'].astype('float32').squeeze()\nlat_1 = HRRR_proj.loc['lat_1'].astype('float32').squeeze()\nlat_2 = HRRR_proj.loc['lat_2'].astype('float32').squeeze()\nlon_0 = HRRR_proj.loc['lon_0'].astype('float32').squeeze()\na = HRRR_proj.loc['a'].astype('float32').squeeze()\nb = HRRR_proj.loc['b'].astype('float32').squeeze()\n\nprojData= ccrs.LambertConformal(central_longitude=lon_0, central_latitude=lat_0,\n                                standard_parallels=[lat_1,lat_2],\n                                globe=ccrs.Globe(semimajor_axis=a, semiminor_axis=b))\n\nNote: The HRRR's projection assumes a spherical earth, whose semi-major/minor axes are both equal to 6371.229 km. We therefore need to explicitly define a Globe in Cartopy with these values.\n\nExamine the dataset’s coordinate variables. Each x- and y- value represents distance in meters from the central latitude and longitude.\n\nds.coords\n\nCreate an object pointing to the dataset’s data variable.\n\nairTemp = ds.TMP\n\nWhen we examine the object, we see that it is a special type of DataArray ... a DaskArray.\n\nairTemp\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#access-archived-hrrr-data-hosted-on-aws-in-zarr-format","position":13},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Sidetrip: Dask"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#sidetrip-dask","position":14},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Sidetrip: Dask"},"content":"\n\nDask is a Python library that is especially well-suited for handling very large datasets (especially those that are too large to fit into RAM) and is nicely integrated with Xarray. We're going to defer a detailed exploration of Dask for now. But suffice it to say that when we use open_mfdataset, the resulting objects are Dask objects.\n\nMetPy supports Dask arrays, and so performing a unit conversion is straightforward.\n\nairTemp = airTemp.metpy.convert_units('degC')\n\nVerify that the object has the unit change\n\nairTemp\n\nSimilar to what we did for datasets whose projection-related coordinates were latitude and longitude, we define objects pointing to x and y now, so we can pass them to the plotting functions.\n\nx = airTemp.projection_x_coordinate\ny = airTemp.projection_y_coordinate\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#sidetrip-dask","position":15},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Visualize 2m temperature at an analysis time"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#visualize-2m-temperature-at-an-analysis-time","position":16},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Visualize 2m temperature at an analysis time"},"content":"\n\nFirst, just use Xarray’s plot function to get a quick look to verify that things look right.\n\nairTemp.plot(figsize=(11,8.5))\n\nTo facilitate the bounds of the contour intervals, obtain the min and max values from this DataArray.\n\nA Dask array is even more lazy in terms of its data loading than a basic DataArray in Xarray. If we want to perform a computation on this array, e.g. calculate the mean, min, or max, note that we don't get a result straightaway ... we get another Dask array.\n\nairTemp.min()\n\nWith Dask arrays, applying the min and max functions doesn't actually do the computation ... instead, it is creating a task graph which describes how the computations would be launched. You will need to call Dask's compute function to actually trigger the computation.\n\nminTemp = airTemp.min().compute()\nmaxTemp = airTemp.max().compute()\n\nminTemp.values, maxTemp.values\n\nBased on the min and max, define a range of values used for contouring. Let’s invoke NumPy’s floor and ceil(ing) functions so these values conform to whatever variable we are contouring.\n\nfint = np.arange(np.floor(minTemp.values),np.ceil(maxTemp.values) + 2, 2)\n\nfint\n\nFor a single map, setting the contour fill values as we did above is appropriate. But if you were producing a series of maps that span a range of times, a consistent (and thus wider) range of values would be better.\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#visualize-2m-temperature-at-an-analysis-time","position":17},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Plot the map"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#plot-the-map","position":18},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Plot the map"},"content":"We’ll define the plot extent to nicely encompass the HRRR’s spatial domain.\n\nlatN = 50.4\nlatS = 24.25\nlonW = -123.8\nlonE = -71.2\n\nres = '50m'\n\nfig = plt.figure(figsize=(18,12))\nax = fig.add_subplot(1,1,1,projection=projData)\nax.set_extent ([lonW,lonE,latS,latN],crs=ccrs.PlateCarree())\nax.add_feature(cfeature.COASTLINE.with_scale(res))\nax.add_feature(cfeature.STATES.with_scale(res))\n\n# Add the title\ntl1 = 'HRRR 2m temperature (°C)'\ntl2 = f'Analysis valid at: {hour}00 UTC {date}'  \nax.set_title(f'{tl1}\\n{tl2}',fontsize=16)\n# Contour fill\nCF = ax.contourf(x,y,airTemp,levels=fint,cmap=plt.get_cmap('coolwarm'))\n# Make a colorbar for the ContourSet returned by the contourf call.\ncbar = fig.colorbar(CF,shrink=0.5)\ncbar.set_label(f'2m Temperature (°C)', size='large')\n\n","type":"content","url":"/notebooks/example-workflows/plot-2mt#plot-the-map","position":19},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#summary","position":20},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Summary"},"content":"Xarray can read gridded datasets in Zarr format, which is ideal for a cloud-based object store system such as S3.\n\nXarray and MetPy both support Dask, a library that is particularly well-suited for very large datasets.","type":"content","url":"/notebooks/example-workflows/plot-2mt#summary","position":21},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"What’s next?"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#whats-next","position":22},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"What’s next?"},"content":"On your own, browse the \n\nhrrrzarr S3 bucket. Try making maps for different variables and/or different times.","type":"content","url":"/notebooks/example-workflows/plot-2mt#whats-next","position":23},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Resources and References"},"type":"lvl2","url":"/notebooks/example-workflows/plot-2mt#resources-and-references","position":24},{"hierarchy":{"lvl1":"Plotting HRRR 2-meter temperatures","lvl2":"Resources and References"},"content":"HRRR in Zarr format\n\nNCEP’s HRRR S3 archive (GRIB format)\n\nWhat is object store?\n\nXarray’s Dask implementation","type":"content","url":"/notebooks/example-workflows/plot-2mt#resources-and-references","position":25},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in Project Pythia’s HRRR-AWS Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}